// ğŸ”¶ DOC-010: Token analysis engine - ğŸ” Core analysis and suggestion logic
package main

import (
	"bufio"
	"fmt"
	"go/ast"
	"go/parser"
	"go/token"
	"os"
	"path/filepath"
	"regexp"
	"sort"
	"strconv"
	"strings"
	"time"
)

// ğŸ”¶ DOC-010: Core token analyzer - ğŸ” Main analysis engine
type TokenAnalyzer struct {
	config     *AnalysisConfig
	fileSet    *token.FileSet
	featureMap map[string]FeatureMapping
}

// ğŸ”¶ DOC-010: Token validator - ğŸ›¡ï¸ Standards compliance validator
type TokenValidator struct {
	config *AnalysisConfig
}

// ğŸ”¶ DOC-010: Batch processor - ğŸš€ Mass processing engine
type BatchProcessor struct {
	analyzer  *TokenAnalyzer
	validator *TokenValidator
}

// ğŸ”¶ DOC-010: Analyzer constructor - ğŸ”§ Initialize analysis engine
func NewTokenAnalyzer() *TokenAnalyzer {
	config := DefaultAnalysisConfig()
	analyzer := &TokenAnalyzer{
		config:  config,
		fileSet: token.NewFileSet(),
	}

	// ğŸ”¶ DOC-010: Feature tracking integration - ğŸ¯ Load feature mappings
	if err := analyzer.loadFeatureMappings(); err != nil {
		// ğŸ”¶ DOC-010: Fallback to empty mapping - ğŸ›¡ï¸ Graceful degradation
		analyzer.featureMap = make(map[string]FeatureMapping)
	}

	return analyzer
}

// ğŸ”¶ DOC-010: Validator constructor - ğŸ›¡ï¸ Initialize validation engine
func NewTokenValidator() *TokenValidator {
	return &TokenValidator{
		config: DefaultAnalysisConfig(),
	}
}

// ğŸ”¶ DOC-010: Batch processor constructor - ğŸš€ Initialize batch processing
func NewBatchProcessor() *BatchProcessor {
	return &BatchProcessor{
		analyzer:  NewTokenAnalyzer(),
		validator: NewTokenValidator(),
	}
}

// ğŸ”¶ DOC-010: Feature mapping loader - ğŸ¯ Feature tracking integration
func (ta *TokenAnalyzer) loadFeatureMappings() error {
	ta.featureMap = make(map[string]FeatureMapping)

	// ğŸ”¶ DOC-010: Feature tracking file processing - ğŸ“ Parse feature tracking
	file, err := os.Open(ta.config.FeatureTrackingFile)
	if err != nil {
		return fmt.Errorf("failed to open feature tracking file: %w", err)
	}
	defer file.Close()

	scanner := bufio.NewScanner(file)
	featureRegex := regexp.MustCompile(`\|\s*([A-Z]+-[0-9]+)\s*\|.*\|\s*(â­|ğŸ”º|ğŸ”¶|ğŸ”»)\s*(CRITICAL|HIGH|MEDIUM|LOW)`)

	// ğŸ”¶ DOC-010: Feature extraction from table - ğŸ” Parse feature entries
	for scanner.Scan() {
		line := scanner.Text()
		matches := featureRegex.FindStringSubmatch(line)
		if len(matches) >= 4 {
			featureID := matches[1]
			priorityIcon := matches[2]
			priorityText := matches[3]

			ta.featureMap[featureID] = FeatureMapping{
				FeatureID:   featureID,
				Priority:    priorityText,
				Status:      "UNKNOWN", // Could be enhanced to parse status
				Description: "",        // Could be enhanced to parse description
				Category:    strings.Split(featureID, "-")[0],
			}

			// ğŸ”¶ DOC-010: Priority icon validation - ğŸ›¡ï¸ Consistency checking
			if priorityIcon != "" {
				// Store the icon for consistency checking
				if ta.featureMap[featureID].Priority == "" {
					mapping := ta.featureMap[featureID]
					mapping.Priority = priorityText
					ta.featureMap[featureID] = mapping
				}
			}
		}
	}

	return scanner.Err()
}

// ğŸ”¶ DOC-010: Target analysis - ğŸ” Analyze directory or file
func (ta *TokenAnalyzer) AnalyzeTarget(target string) (*AnalysisResults, error) {
	startTime := time.Now()

	// ğŸ”¶ DOC-010: Target validation - ğŸ›¡ï¸ Input validation
	info, err := os.Stat(target)
	if err != nil {
		return nil, fmt.Errorf("target not found: %w", err)
	}

	results := &AnalysisResults{
		Target:            target,
		FunctionsAnalyzed: 0,
		MissingTokens:     0,
		FormatViolations:  0,
		Suggestions:       make([]TokenSuggestion, 0),
		AnalysisTimestamp: time.Now(),
	}

	// ğŸ”¶ DOC-010: Directory vs file processing - ğŸ”§ Path handling
	if info.IsDir() {
		err = ta.analyzeDirectory(target, results)
	} else {
		err = ta.analyzeFile(target, results)
	}

	if err != nil {
		return nil, fmt.Errorf("analysis failed: %w", err)
	}

	// ğŸ”¶ DOC-010: Results post-processing - ğŸ“Š Statistics calculation
	ta.calculateStatistics(results)
	results.ProcessingTime = time.Since(startTime)

	return results, nil
}

// ğŸ”¶ DOC-010: Directory analysis - ğŸ” Recursive directory processing
func (ta *TokenAnalyzer) analyzeDirectory(dir string, results *AnalysisResults) error {
	return filepath.Walk(dir, func(path string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		// ğŸ”¶ DOC-010: Go file filtering - ğŸ“ File type validation
		if !info.IsDir() && strings.HasSuffix(path, ".go") {
			// Skip test files unless explicitly requested
			if strings.HasSuffix(path, "_test.go") {
				return nil
			}

			return ta.analyzeFile(path, results)
		}

		return nil
	})
}

// ğŸ”¶ DOC-010: File analysis - ğŸ” Individual file processing
func (ta *TokenAnalyzer) analyzeFile(filePath string, results *AnalysisResults) error {
	// ğŸ”¶ DOC-010: AST parsing - ğŸ” Code structure analysis
	src, err := os.ReadFile(filePath)
	if err != nil {
		return fmt.Errorf("failed to read file %s: %w", filePath, err)
	}

	file, err := parser.ParseFile(ta.fileSet, filePath, src, parser.ParseComments)
	if err != nil {
		return fmt.Errorf("failed to parse file %s: %w", filePath, err)
	}

	// ğŸ”¶ DOC-010: Function discovery - ğŸ” Function extraction
	ast.Inspect(file, func(n ast.Node) bool {
		switch node := n.(type) {
		case *ast.FuncDecl:
			if node.Name != nil && node.Name.IsExported() {
				results.FunctionsAnalyzed++

				// ğŸ”¶ DOC-010: Function analysis - ğŸ” Individual function processing
				suggestion, err := ta.analyzeFunctionDecl(filePath, node, src)
				if err == nil && suggestion != nil {
					results.Suggestions = append(results.Suggestions, *suggestion)
				}
			}
		}
		return true
	})

	return nil
}

// ğŸ”¶ DOC-010: Function declaration analysis - ğŸ” Function-specific analysis
func (ta *TokenAnalyzer) analyzeFunctionDecl(filePath string, funcDecl *ast.FuncDecl, src []byte) (*TokenSuggestion, error) {
	// ğŸ”¶ DOC-010: Function signature extraction - ğŸ“ Signature analysis
	signature := ta.extractFunctionSignature(funcDecl)

	// ğŸ”¶ DOC-010: Line number calculation - ğŸ“ Position tracking
	position := ta.fileSet.Position(funcDecl.Pos())

	// ğŸ”¶ DOC-010: Context analysis - ğŸ” Surrounding code analysis
	context, err := ta.analyzeContext(filePath, position.Line, src)
	if err != nil {
		return nil, fmt.Errorf("context analysis failed: %w", err)
	}

	// ğŸ”¶ DOC-010: Token suggestion generation - ğŸ’¡ Suggestion creation
	suggestion := &TokenSuggestion{
		FilePath:          filePath,
		LineNumber:        position.Line,
		FunctionName:      signature.Name,
		FunctionSignature: signature,
		Context:           context,
		Timestamp:         time.Now(),
	}

	// ğŸ”¶ DOC-010: Priority assignment - â­ Priority determination
	suggestion.PriorityIcon, suggestion.PriorityReason = ta.determinePriority(signature, context)

	// ğŸ”¶ DOC-010: Action assignment - ğŸ”§ Action determination
	suggestion.ActionIcon, suggestion.ActionReason = ta.determineAction(signature, context)

	// ğŸ”¶ DOC-010: Feature mapping - ğŸ¯ Feature ID assignment
	suggestion.FeatureID = ta.determineFeatureID(signature, context)

	// ğŸ”¶ DOC-010: Confidence calculation - ğŸ“Š Quality scoring
	suggestion.Confidence = ta.calculateConfidence(signature, context, suggestion)

	// ğŸ”¶ DOC-010: Complexity analysis - ğŸ“Š Complexity determination
	suggestion.ComplexityLevel = ta.analyzeComplexity(funcDecl)

	// ğŸ”¶ DOC-010: Token format generation - ğŸ“ Final token creation
	suggestion.SuggestedToken = fmt.Sprintf("// %s %s: %s - %s %s",
		suggestion.PriorityIcon,
		suggestion.FeatureID,
		ta.generateDescription(signature, context),
		suggestion.ActionIcon,
		suggestion.ActionReason)

	return suggestion, nil
}

// ğŸ”¶ DOC-010: Function signature extraction - ğŸ“ Signature parsing
func (ta *TokenAnalyzer) extractFunctionSignature(funcDecl *ast.FuncDecl) FunctionSignature {
	signature := FunctionSignature{
		Name:       funcDecl.Name.Name,
		IsExported: funcDecl.Name.IsExported(),
		Parameters: make([]Parameter, 0),
	}

	// ğŸ”¶ DOC-010: Receiver analysis - ğŸ” Method receiver extraction
	if funcDecl.Recv != nil {
		for _, field := range funcDecl.Recv.List {
			if len(field.Names) > 0 {
				signature.Receiver = field.Names[0].Name
			}
		}
	}

	// ğŸ”¶ DOC-010: Parameter extraction - ğŸ“ Parameter analysis
	if funcDecl.Type.Params != nil {
		for _, field := range funcDecl.Type.Params.List {
			paramType := "unknown"
			if field.Type != nil {
				paramType = fmt.Sprintf("%T", field.Type)
			}

			if len(field.Names) > 0 {
				for _, name := range field.Names {
					signature.Parameters = append(signature.Parameters, Parameter{
						Name: name.Name,
						Type: paramType,
					})
				}
			} else {
				signature.Parameters = append(signature.Parameters, Parameter{
					Name: "",
					Type: paramType,
				})
			}
		}
	}

	// ğŸ”¶ DOC-010: Return type extraction - ğŸ“ Return type analysis
	if funcDecl.Type.Results != nil {
		returnTypes := make([]string, 0)
		for _, field := range funcDecl.Type.Results.List {
			if field.Type != nil {
				returnTypes = append(returnTypes, fmt.Sprintf("%T", field.Type))
			}
		}
		signature.ReturnType = strings.Join(returnTypes, ", ")
	} else {
		signature.ReturnType = "void"
	}

	return signature
}

// ğŸ”¶ DOC-010: Context analysis - ğŸ” Surrounding code analysis
func (ta *TokenAnalyzer) analyzeContext(filePath string, lineNumber int, src []byte) (map[string]string, error) {
	context := make(map[string]string)

	// ğŸ”¶ DOC-010: Source code line extraction - ğŸ“ Line-based analysis
	lines := strings.Split(string(src), "\n")
	if lineNumber > len(lines) {
		return context, fmt.Errorf("line number %d exceeds file length", lineNumber)
	}

	// ğŸ”¶ DOC-010: Context window extraction - ğŸ” Surrounding code collection
	start := max(0, lineNumber-5)
	end := min(len(lines), lineNumber+5)

	contextLines := lines[start:end]
	context["surrounding_code"] = strings.Join(contextLines, "\n")

	// ğŸ”¶ DOC-010: Pattern matching - ğŸ” Code pattern recognition
	fullCode := strings.Join(lines, "\n")

	// ğŸ”¶ DOC-010: Error handling detection - ğŸ›¡ï¸ Error pattern analysis
	if strings.Contains(fullCode, "error") || strings.Contains(fullCode, "err") {
		context["error_handling"] = "true"
	}

	// ğŸ”¶ DOC-010: Resource management detection - ğŸ›¡ï¸ Resource pattern analysis
	if strings.Contains(fullCode, "defer") || strings.Contains(fullCode, "Close") {
		context["resource_management"] = "true"
	}

	// ğŸ”¶ DOC-010: Configuration detection - ğŸ”§ Config pattern analysis
	if strings.Contains(fullCode, "config") || strings.Contains(fullCode, "Config") {
		context["configuration_related"] = "true"
	}

	// ğŸ”¶ DOC-010: File operation detection - ğŸ” File pattern analysis
	if strings.Contains(fullCode, "file") || strings.Contains(fullCode, "File") {
		context["file_operations"] = "true"
	}

	return context, nil
}

// ğŸ”¶ DOC-010: Priority determination - â­ Priority assignment logic
func (ta *TokenAnalyzer) determinePriority(signature FunctionSignature, context map[string]string) (string, string) {
	funcName := strings.ToLower(signature.Name)

	// ğŸ”¶ DOC-010: Critical priority patterns - â­ Critical function detection
	for _, pattern := range ta.config.PriorityRules.CriticalPatterns {
		if strings.Contains(funcName, strings.ToLower(pattern)) {
			return "â­", fmt.Sprintf("Critical operation: %s", pattern)
		}
	}

	// ğŸ”¶ DOC-010: High priority patterns - ğŸ”º High priority detection
	for _, pattern := range ta.config.PriorityRules.HighPatterns {
		if strings.Contains(funcName, strings.ToLower(pattern)) {
			return "ğŸ”º", fmt.Sprintf("High priority: %s", pattern)
		}
	}

	// ğŸ”¶ DOC-010: Medium priority patterns - ğŸ”¶ Medium priority detection
	for _, pattern := range ta.config.PriorityRules.MediumPatterns {
		if strings.Contains(funcName, strings.ToLower(pattern)) {
			return "ğŸ”¶", fmt.Sprintf("Medium priority: %s", pattern)
		}
	}

	// ğŸ”¶ DOC-010: Context-based priority adjustment - ğŸ” Context priority analysis
	if context["error_handling"] == "true" {
		return "ğŸ”º", "High priority: error handling function"
	}

	if context["resource_management"] == "true" {
		return "ğŸ”º", "High priority: resource management function"
	}

	// ğŸ”¶ DOC-010: Default low priority - ğŸ”» Default assignment
	return "ğŸ”»", "Low priority: utility function"
}

// ğŸ”¶ DOC-010: Action determination - ğŸ”§ Action assignment logic
func (ta *TokenAnalyzer) determineAction(signature FunctionSignature, context map[string]string) (string, string) {
	funcName := strings.ToLower(signature.Name)

	// ğŸ”¶ DOC-010: Analysis action patterns - ğŸ” Analysis function detection
	for _, pattern := range ta.config.ActionRules.AnalysisPatterns {
		if strings.Contains(funcName, strings.ToLower(pattern)) {
			return "ğŸ”", fmt.Sprintf("Analysis operation: %s", pattern)
		}
	}

	// ğŸ”¶ DOC-010: Documentation action patterns - ğŸ“ Documentation function detection
	for _, pattern := range ta.config.ActionRules.DocumentationPatterns {
		if strings.Contains(funcName, strings.ToLower(pattern)) {
			return "ğŸ“", fmt.Sprintf("Documentation operation: %s", pattern)
		}
	}

	// ğŸ”¶ DOC-010: Configuration action patterns - ğŸ”§ Configuration function detection
	for _, pattern := range ta.config.ActionRules.ConfigurationPatterns {
		if strings.Contains(funcName, strings.ToLower(pattern)) {
			return "ğŸ”§", fmt.Sprintf("Configuration operation: %s", pattern)
		}
	}

	// ğŸ”¶ DOC-010: Protection action patterns - ğŸ›¡ï¸ Protection function detection
	for _, pattern := range ta.config.ActionRules.ProtectionPatterns {
		if strings.Contains(funcName, strings.ToLower(pattern)) {
			return "ğŸ›¡ï¸", fmt.Sprintf("Protection operation: %s", pattern)
		}
	}

	// ğŸ”¶ DOC-010: Context-based action assignment - ğŸ” Context action analysis
	if context["error_handling"] == "true" {
		return "ğŸ›¡ï¸", "Protection: error handling"
	}

	if context["configuration_related"] == "true" {
		return "ğŸ”§", "Configuration: config management"
	}

	// ğŸ”¶ DOC-010: Default configuration action - ğŸ”§ Default assignment
	return "ğŸ”§", "Configuration: general operation"
}

// ğŸ”¶ DOC-010: Feature ID determination - ğŸ¯ Feature mapping logic
func (ta *TokenAnalyzer) determineFeatureID(signature FunctionSignature, context map[string]string) string {
	funcName := strings.ToLower(signature.Name)

	// ğŸ”¶ DOC-010: Direct feature mapping - ğŸ¯ Feature tracking lookup
	for featureID, mapping := range ta.featureMap {
		category := strings.ToLower(mapping.Category)
		if strings.Contains(funcName, category) {
			return featureID
		}
	}

	// ğŸ”¶ DOC-010: Category-based assignment - ğŸ¯ Category inference
	if strings.Contains(funcName, "config") {
		return "CFG-NEW"
	}
	if strings.Contains(funcName, "archive") {
		return "ARCH-NEW"
	}
	if strings.Contains(funcName, "backup") {
		return "FILE-NEW"
	}
	if strings.Contains(funcName, "git") {
		return "GIT-NEW"
	}
	if strings.Contains(funcName, "test") {
		return "TEST-NEW"
	}

	// ğŸ”¶ DOC-010: Default generic assignment - ğŸ¯ Fallback assignment
	return "UTIL-NEW"
}

// ğŸ”¶ DOC-010: Confidence calculation - ğŸ“Š Quality scoring algorithm
func (ta *TokenAnalyzer) calculateConfidence(signature FunctionSignature, context map[string]string, suggestion *TokenSuggestion) float64 {
	confidence := 0.0
	weights := ta.config.ConfidenceWeights

	// ğŸ”¶ DOC-010: Signature matching confidence - ğŸ“ Signature quality assessment
	if signature.IsExported && len(signature.Parameters) > 0 {
		confidence += weights.SignatureMatch
	}

	// ğŸ”¶ DOC-010: Pattern matching confidence - ğŸ” Pattern recognition quality
	if suggestion.PriorityReason != "Low priority: utility function" {
		confidence += weights.PatternMatch
	}

	// ğŸ”¶ DOC-010: Context matching confidence - ğŸ” Context analysis quality
	if len(context) > 2 {
		confidence += weights.ContextMatch
	}

	// ğŸ”¶ DOC-010: Feature mapping confidence - ğŸ¯ Feature tracking quality
	if !strings.HasSuffix(suggestion.FeatureID, "-NEW") {
		confidence += weights.FeatureMapping
	}

	// ğŸ”¶ DOC-010: Complexity factor - ğŸ“Š Complexity-based adjustment
	if len(signature.Parameters) > 3 || signature.ReturnType != "void" {
		confidence += weights.ComplexityFactor
	}

	// ğŸ”¶ DOC-010: Confidence normalization - ğŸ“Š Score normalization
	return minFloat64(1.0, confidence)
}

// ğŸ”¶ DOC-010: Complexity analysis - ğŸ“Š Function complexity assessment
func (ta *TokenAnalyzer) analyzeComplexity(funcDecl *ast.FuncDecl) string {
	paramCount := 0
	if funcDecl.Type.Params != nil {
		paramCount = len(funcDecl.Type.Params.List)
	}

	returnCount := 0
	if funcDecl.Type.Results != nil {
		returnCount = len(funcDecl.Type.Results.List)
	}

	// ğŸ”¶ DOC-010: Complexity scoring - ğŸ“Š Simple complexity heuristics
	complexityScore := paramCount + returnCount

	if complexityScore >= 8 {
		return "CRITICAL"
	} else if complexityScore >= 5 {
		return "HIGH"
	} else if complexityScore >= 3 {
		return "MEDIUM"
	}

	return "LOW"
}

// ğŸ”¶ DOC-010: Description generation - ğŸ“ Human-readable description
func (ta *TokenAnalyzer) generateDescription(signature FunctionSignature, context map[string]string) string {
	funcName := signature.Name

	// ğŸ”¶ DOC-010: Context-based description - ğŸ“ Contextual description generation
	if context["error_handling"] == "true" {
		return fmt.Sprintf("%s with error handling", funcName)
	}

	if context["resource_management"] == "true" {
		return fmt.Sprintf("%s with resource management", funcName)
	}

	if context["configuration_related"] == "true" {
		return fmt.Sprintf("%s configuration operation", funcName)
	}

	// ğŸ”¶ DOC-010: Default description - ğŸ“ Generic description
	return fmt.Sprintf("%s implementation", funcName)
}

// ğŸ”¶ DOC-010: Statistics calculation - ğŸ“Š Results post-processing
func (ta *TokenAnalyzer) calculateStatistics(results *AnalysisResults) {
	if len(results.Suggestions) == 0 {
		return
	}

	// ğŸ”¶ DOC-010: Confidence statistics - ğŸ“ˆ Confidence analysis
	var totalConfidence float64
	minConf := 1.0
	maxConf := 0.0
	highCount := 0
	mediumCount := 0
	lowCount := 0

	for _, suggestion := range results.Suggestions {
		totalConfidence += suggestion.Confidence
		minConf = minFloat64(minConf, suggestion.Confidence)
		maxConf = maxFloat64(maxConf, suggestion.Confidence)

		if suggestion.Confidence > 0.8 {
			highCount++
		} else if suggestion.Confidence > 0.5 {
			mediumCount++
		} else {
			lowCount++
		}
	}

	results.ConfidenceStats = ConfidenceStats{
		AverageConfidence: totalConfidence / float64(len(results.Suggestions)),
		MinConfidence:     minConf,
		MaxConfidence:     maxConf,
		HighConfidence:    highCount,
		MediumConfidence:  mediumCount,
		LowConfidence:     lowCount,
	}

	// ğŸ”¶ DOC-010: Suggestion sorting - ğŸ“Š Quality-based ordering
	sort.Slice(results.Suggestions, func(i, j int) bool {
		return results.Suggestions[i].Confidence > results.Suggestions[j].Confidence
	})
}

// ğŸ”¶ DOC-010: Function-specific suggestion - ğŸ” Individual function analysis
func (ta *TokenAnalyzer) SuggestForFunction(filePath, lineStr string) (*TokenSuggestion, error) {
	lineNumber, err := strconv.Atoi(lineStr)
	if err != nil {
		return nil, fmt.Errorf("invalid line number: %w", err)
	}

	// ğŸ”¶ DOC-010: File parsing for specific function - ğŸ” Targeted analysis
	src, err := os.ReadFile(filePath)
	if err != nil {
		return nil, fmt.Errorf("failed to read file: %w", err)
	}

	file, err := parser.ParseFile(ta.fileSet, filePath, src, parser.ParseComments)
	if err != nil {
		return nil, fmt.Errorf("failed to parse file: %w", err)
	}

	// ğŸ”¶ DOC-010: Function location matching - ğŸ“ Position-based lookup
	var targetFunc *ast.FuncDecl
	ast.Inspect(file, func(n ast.Node) bool {
		if funcDecl, ok := n.(*ast.FuncDecl); ok {
			pos := ta.fileSet.Position(funcDecl.Pos())
			if pos.Line <= lineNumber && lineNumber <= ta.fileSet.Position(funcDecl.End()).Line {
				targetFunc = funcDecl
				return false
			}
		}
		return true
	})

	if targetFunc == nil {
		return nil, fmt.Errorf("no function found at line %d", lineNumber)
	}

	// ğŸ”¶ DOC-010: Targeted function analysis - ğŸ” Single function processing
	return ta.analyzeFunctionDecl(filePath, targetFunc, src)
}

// ğŸ”¶ DOC-010: Token validation - ğŸ›¡ï¸ Standards compliance checking
func (tv *TokenValidator) ValidateTokens(directory string) ([]TokenViolation, error) {
	violations := make([]TokenViolation, 0)

	// ğŸ”¶ DOC-010: Directory traversal for validation - ğŸ›¡ï¸ Comprehensive validation
	err := filepath.Walk(directory, func(path string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(path, ".go") {
			fileViolations, err := tv.validateFile(path)
			if err != nil {
				return err
			}
			violations = append(violations, fileViolations...)
		}

		return nil
	})

	return violations, err
}

// ğŸ”¶ DOC-010: File validation - ğŸ›¡ï¸ Individual file compliance checking
func (tv *TokenValidator) validateFile(filePath string) ([]TokenViolation, error) {
	violations := make([]TokenViolation, 0)

	// ğŸ”¶ DOC-010: File content analysis - ğŸ“ Token extraction and validation
	content, err := os.ReadFile(filePath)
	if err != nil {
		return nil, fmt.Errorf("failed to read file: %w", err)
	}

	lines := strings.Split(string(content), "\n")
	tokenRegex := regexp.MustCompile(`//\s*([â­ğŸ”ºğŸ”¶ğŸ”»])?\s*([A-Z]+-[0-9]+):?\s*(.*)`)

	// ğŸ”¶ DOC-010: Line-by-line token validation - ğŸ›¡ï¸ Format compliance checking
	for i, line := range lines {
		if strings.Contains(line, "//") && (strings.Contains(line, "-") || strings.Contains(line, ":")) {
			matches := tokenRegex.FindStringSubmatch(line)
			if len(matches) < 4 {
				violations = append(violations, TokenViolation{
					FilePath:      filePath,
					LineNumber:    i + 1,
					ViolationType: "INVALID_FORMAT",
					CurrentToken:  strings.TrimSpace(line),
					SuggestedFix:  "// ğŸ”¶ FEATURE-ID: Description - ğŸ”§ Action description",
					Severity:      "WARNING",
					RuleID:        "FORMAT-001",
					Description:   "Token does not match required format",
				})
			}
		}
	}

	return violations, nil
}

// ğŸ”¶ DOC-010: Batch processing - ğŸš€ Directory-wide analysis
func (bp *BatchProcessor) ProcessDirectory(directory string) (*BatchResults, error) {
	startTime := time.Now()

	results := &BatchResults{
		Directory:         directory,
		FilesProcessed:    0,
		TotalFunctions:    0,
		FileResults:       make(map[string]FileResult),
		PriorityBreakdown: PriorityBreakdown{},
		ActionBreakdown:   ActionBreakdown{},
		TopSuggestions:    make([]TokenSuggestion, 0),
	}

	// ğŸ”¶ DOC-010: Directory traversal for batch processing - ğŸš€ Comprehensive processing
	err := filepath.Walk(directory, func(path string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(path, ".go") && !strings.HasSuffix(path, "_test.go") {
			fileResult, err := bp.processFile(path)
			if err != nil {
				return err
			}

			results.FilesProcessed++
			results.TotalFunctions += fileResult.FunctionsFound
			results.TotalSuggestions += fileResult.SuggestionsCount
			results.TotalViolations += fileResult.ViolationsCount
			results.FileResults[path] = *fileResult

			// ğŸ”¶ DOC-010: Priority and action breakdown - ğŸ“Š Category statistics
			for _, suggestion := range fileResult.Suggestions {
				bp.updateBreakdowns(suggestion, results)
				results.TopSuggestions = append(results.TopSuggestions, suggestion)
			}
		}

		return nil
	})

	if err != nil {
		return nil, err
	}

	// ğŸ”¶ DOC-010: Results finalization - ğŸ“Š Final processing
	bp.finalizeResults(results)
	results.ProcessingTime = time.Since(startTime)

	return results, nil
}

// ğŸ”¶ DOC-010: Individual file processing for batch - ğŸ” File-specific batch processing
func (bp *BatchProcessor) processFile(filePath string) (*FileResult, error) {
	startTime := time.Now()

	// ğŸ”¶ DOC-010: Combined analysis and validation - ğŸ” Comprehensive file processing
	analysisResults, err := bp.analyzer.AnalyzeTarget(filePath)
	if err != nil {
		return nil, err
	}

	violations, err := bp.validator.validateFile(filePath)
	if err != nil {
		return nil, err
	}

	return &FileResult{
		FilePath:         filePath,
		FunctionsFound:   analysisResults.FunctionsAnalyzed,
		SuggestionsCount: len(analysisResults.Suggestions),
		ViolationsCount:  len(violations),
		Suggestions:      analysisResults.Suggestions,
		Violations:       violations,
		ProcessingTime:   time.Since(startTime),
	}, nil
}

// ğŸ”¶ DOC-010: Breakdown updates - ğŸ“Š Category statistics maintenance
func (bp *BatchProcessor) updateBreakdowns(suggestion TokenSuggestion, results *BatchResults) {
	// ğŸ”¶ DOC-010: Priority breakdown updates - â­ Priority statistics
	switch suggestion.PriorityIcon {
	case "â­":
		results.PriorityBreakdown.Critical++
	case "ğŸ”º":
		results.PriorityBreakdown.High++
	case "ğŸ”¶":
		results.PriorityBreakdown.Medium++
	case "ğŸ”»":
		results.PriorityBreakdown.Low++
	}

	// ğŸ”¶ DOC-010: Action breakdown updates - ğŸ”§ Action statistics
	switch suggestion.ActionIcon {
	case "ğŸ”":
		results.ActionBreakdown.Analysis++
	case "ğŸ“":
		results.ActionBreakdown.Documentation++
	case "ğŸ”§":
		results.ActionBreakdown.Configuration++
	case "ğŸ›¡ï¸":
		results.ActionBreakdown.Protection++
	}
}

// ğŸ”¶ DOC-010: Results finalization - ğŸ“Š Final results processing
func (bp *BatchProcessor) finalizeResults(results *BatchResults) {
	// ğŸ”¶ DOC-010: Top suggestions sorting - ğŸ“Š Quality-based ranking
	sort.Slice(results.TopSuggestions, func(i, j int) bool {
		return results.TopSuggestions[i].Confidence > results.TopSuggestions[j].Confidence
	})

	// ğŸ”¶ DOC-010: Top suggestions limiting - ğŸ“Š Results optimization
	if len(results.TopSuggestions) > 10 {
		results.TopSuggestions = results.TopSuggestions[:10]
	}
}

// ğŸ”¶ DOC-010: Utility functions - ğŸ”§ Helper functions
func min(a, b int) int {
	if a < b {
		return a
	}
	return b
}

func max(a, b int) int {
	if a > b {
		return a
	}
	return b
}

func minFloat64(a, b float64) float64 {
	if a < b {
		return a
	}
	return b
}

func maxFloat64(a, b float64) float64 {
	if a > b {
		return a
	}
	return b
}
